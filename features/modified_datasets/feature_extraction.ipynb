{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6a7b3a0l2ep4"
      },
      "source": [
        "## 1. load the raw dataset\n",
        "* mon_standard.pkl > array code\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "5mfwrTwPtd36"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading datafile...\n",
            "Total samples: 19000\n"
          ]
        }
      ],
      "source": [
        "import pickle\n",
        "\n",
        "USE_SUBLABEL = False\n",
        "URL_PER_SITE = 10\n",
        "TOTAL_URLS   = 950\n",
        "\n",
        "# Load the pickle file\n",
        "print(\"Loading datafile...\")\n",
        "with open(\"../datasets/mon_standard.pkl\", 'rb') as fi: # Path to mon_standard.pkl in Colab\n",
        "    data = pickle.load(fi)\n",
        "\n",
        "X1 = [] # Array to store instances (timestamps) - 19,000 instances, e.g., [[0.0, 0.5, 3.4, ...], [0.0, 4.5, ...], [0.0, 1.5, ...], ... [... ,45.8]]\n",
        "X2 = [] # Array to store instances (direction*size) - size information\n",
        "y = [] # Array to store the site of each instance - 19,000 instances, e.g., [0, 0, 0, 0, 0, 0, ..., 94, 94, 94, 94, 94]\n",
        "\n",
        "# Differentiate instances and sites, and store them in the respective x and y arrays\n",
        "# x array (direction*timestamp), y array (site label)\n",
        "for i in range(TOTAL_URLS):\n",
        "    if USE_SUBLABEL:\n",
        "        label = i\n",
        "    else:\n",
        "        label = i // URL_PER_SITE # Calculate which site's URL the current URL being processed belongs to and set that value as the label. Thus, URLs fetched from the same site are labeled identically.\n",
        "    for sample in data[i]:\n",
        "        size_seq = []\n",
        "        time_seq = []\n",
        "        for c in sample:\n",
        "            dr = 1 if c > 0 else -1\n",
        "            time_seq.append(abs(c))\n",
        "            size_seq.append(dr * 512)\n",
        "        X1.append(time_seq)\n",
        "        X2.append(size_seq)\n",
        "        y.append(label)\n",
        "size = len(y)\n",
        "\n",
        "print(f'Total samples: {size}') # Output: 19000\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Implement candidate features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Candidate features\n",
        " \n",
        "# Total packets\n",
        "total_num = []\n",
        "total_sum_dir = []\n",
        "total_avg = []\n",
        "\n",
        "# Incoming packets\n",
        "inpkt_num = []\n",
        "inpkt_avg = []\n",
        "inpkt_sum = []\n",
        "inpkt_num_frac_total = []\n",
        "inpkt_avg_ordering = []\n",
        "inpkt_std_ordering = []\n",
        "inpkt_num_frac_outpkt = []\n",
        "inpkt_sum_firstn = []\n",
        "\n",
        "# Outgoing packets\n",
        "outpkt_num = []\n",
        "outpkt_avg = []\n",
        "outpkt_sum = []\n",
        "outpkt_num_frac_total = []\n",
        "outpkt_avg_ordering = []\n",
        "outpkt_std_ordering = []\n",
        "outpkt_num_frac_inpkt = []\n",
        "outpkt_sum_firstn = []\n",
        "\n",
        "# etc\n",
        "pkt_avg_sec = []\n",
        "pkt_std_sec = []\n",
        "pkt_max_sec = []\n",
        "inpkt_avg_sec = []\n",
        "inpkt_std_sec = []\n",
        "inpkt_max_sec = []\n",
        "outpkt_avg_sec = []\n",
        "outpkt_std_sec = []\n",
        "outpkt_max_sec = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for time, dir in X1, X2:\n",
        "    # Total packets\n",
        "    f1 = len(X1)\n",
        "    total_num.append(f1)\n",
        "    f2 = sum(dir)\n",
        "    total_sum_dir.append(f2)\n",
        "    f3 = np.average(time)\n",
        "    total_avg.append(f3)\n",
        "    \n",
        "    # Incoming, Outgoing packets\n",
        "    inpkt = []\n",
        "    inpkt_index = []\n",
        "    outpkt = []\n",
        "    outpkt_index = []\n",
        "    for i in len(dir):\n",
        "        if(dir[i] < 0):\n",
        "            inpkt.append(time[i])\n",
        "            inpkt_index.append(i)\n",
        "            continue\n",
        "        outpkt.append(time[i])\n",
        "        outpkt_index.append(i)\n",
        "            \n",
        "    f5 = len(inpkt)\n",
        "    inpkt_num.append(f5)\n",
        "    f6 = np.average(inpkt)\n",
        "    inpkt_avg.append(inpkt)\n",
        "    f7 = sum(inpkt)\n",
        "    inpkt_sum.append(f7)\n",
        "    f8 = f5 / f1\n",
        "    inpkt_num_frac_total.append(f8)\n",
        "    f9 = np.average(inpkt_index)\n",
        "    inpkt_avg_ordering.append(f9)\n",
        "    f10 = np.std(inpkt_index)\n",
        "    inpkt_std_ordering.append(f10)\n",
        "    \n",
        "    f13 = len(outpkt)\n",
        "    outpkt_num.append(f13)\n",
        "    f14 = np.average(outpkt)\n",
        "    outpkt_avg.append(f14)\n",
        "    f15 = sum(inpkt)\n",
        "    outpkt_sum.append(f15)\n",
        "    f16 = f13 / f1\n",
        "    outpkt_num_frac_total.append(f16)\n",
        "    f17 = np.average(outpkt_index)\n",
        "    outpkt_avg_ordering.append(f17)\n",
        "    f18 = np.std(outpkt_index)\n",
        "    outpkt_std_ordering.append(f18)\n",
        "    \n",
        "    f11 = f5 / f13\n",
        "    inpkt_num_frac_outpkt.append(f11)\n",
        "    f19 = f13 / f5\n",
        "    outpkt_num_frac_inpkt.append(f19)\n",
        "    \n",
        "    inpkt_sum_firstn.append(f12)\n",
        "    outpkt_sum_firstn.append(f21)\n",
        "    \n",
        "    \n",
        "\n",
        "    #etc\n",
        "    pkt_num_sec = []\n",
        "    inpkt_num_sec = []\n",
        "    outpkt_num_sec = []\n",
        "    start_t = 0\n",
        "    pkt_sum = input_sum = output_sum = 0 \n",
        "    while start_t <= time_seq[-1]: \n",
        "        end_t = start_t + 1 \n",
        "        pkt_in_interval = [i for i, t in enumerate(time) if start_t <= t < end_t]\n",
        "        \n",
        "        pkt_count = len(pkt_in_interval)\n",
        "        pkt_num_sec.append(pkt_count)\n",
        "        inpkt_count = sum(1 for idx in pkt_in_interval if idx in inpkt_index)\n",
        "        inpkt_num_sec.append(inpkt_count)\n",
        "        outpkt_count = sum(1 for idx in pkt_in_interval if idx in outpkt_index)\n",
        "        outpkt_num_sec.append(inpkt_count)\n",
        "        \n",
        "        start_t = end_t\n",
        "\n",
        "    pkt_avg_sec = np.average(pkt_num_sec)\n",
        "    pkt_std_sec = np.std(pkt_num_sec)\n",
        "    pkt_max_sec = max(pkt_num_sec)\n",
        "    inpkt_avg_sec = np.average(inpkt_num_sec)\n",
        "    inpkt_std_sec = np.std(inpkt_num_sec)\n",
        "    inpkt_max_sec = max(inpkt_num_sec)\n",
        "    outpkt_avg_sec = np.average(outpkt_num_sec)\n",
        "    outpkt_std_sec = np.std(outpkt_num_sec)\n",
        "    outpkt_max_sec = max(outpkt_num_sec)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data = {\n",
        "    'total_num': total_num,\n",
        "    'total_sum_dir': total_sum_dir,\n",
        "    'total_avg': total_avg,\n",
        "    'inpkt_num': inpkt_num,\n",
        "    'inpkt_avg': inpkt_avg,\n",
        "    'inpkt_sum': inpkt_sum,\n",
        "    'inpkt_num_frac_total': inpkt_num_frac_total,\n",
        "    'inpkt_avg_ordering': inpkt_avg_ordering,\n",
        "    'inpkt_std_ordering': inpkt_std_ordering,\n",
        "    'inpkt_num_frac_outpkt': inpkt_num_frac_outpkt,\n",
        "    'inpkt_sum_firstn': inpkt_sum_firstn,\n",
        "    'outpkt_num': outpkt_num,\n",
        "    'outpkt_avg': outpkt_avg,\n",
        "    'outpkt_sum': outpkt_sum,\n",
        "    'outpkt_num_frac_total': outpkt_num_frac_total,\n",
        "    'outpkt_avg_ordering': outpkt_avg_ordering,\n",
        "    'outpkt_std_ordering': outpkt_std_ordering,\n",
        "    'outpkt_num_frac_inpkt': outpkt_num_frac_inpkt,\n",
        "    'outpkt_sum_firstn': outpkt_sum_firstn,\n",
        "    'pkt_avg_sec': pkt_avg_sec,\n",
        "    'pkt_std_sec': pkt_std_sec,\n",
        "    'pkt_max_sec': pkt_max_sec,\n",
        "    'inpkt_avg_sec': inpkt_avg_sec,\n",
        "    'inpkt_std_sec': inpkt_std_sec,\n",
        "    'inpkt_max_sec': inpkt_max_sec,\n",
        "    'outpkt_avg_sec': outpkt_avg_sec,\n",
        "    'outpkt_std_sec': outpkt_std_sec,\n",
        "    'outpkt_max_sec': outpkt_max_sec\n",
        "}\n",
        "df = pd.DataFrame(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(df.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. feature 중요도 파악을 위한 RF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import cross_val_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    df, y, test_size=0.2, random_state=1\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "clf = RandomForestClassifier(n_estimators=20, criterion=\"entropy\", max_depth=100, min_samples_split=2, max_features=\"sqrt\", random_state=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "scores = cross_val_score(clf, df, y, cv=5)\n",
        "print(scores)\n",
        "print(sum(scores)/len(scores))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "clf.fit(X_train, y_train)\n",
        "imp_score=clf.feature_importances_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "top_indices=np.argsort(imp_score)[::-1][:5]\n",
        "print(top_indices)\n",
        "top_feature_names=[data.feature_names[int(x)] for x in top_indices]\n",
        "print(top_feature_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "feature_imp = pd.Series(clf.feature_importances_, index=data.feature_names).sort_values(ascending=False)\n",
        "\n",
        "plt.figure(figsize=(10,6))\n",
        "sns.barplot(x=feature_imp, y=feature_imp.index)\n",
        "plt.xlabel('Feature Importance Score')\n",
        "plt.ylabel('Features')\n",
        "plt.title(\"Visualizing Important Features\")\n",
        "plt.tight_layout()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
